# 5 Questions to Ask Any AEO Tool
## Enterprise Buyer's Due Diligence Framework

**Purpose:** Evaluate trustworthiness & enterprise-readiness of AEO platforms  
**Use:** Sales conversations, RFP responses, procurement evaluation  
**Format:** One-page printable reference

---

## 🔍 The Framework

Every AEO tool claims to help you optimize for AI search. But which ones deliver audit-grade results you can trust for strategic decisions?

Ask these 5 questions:

---

## ❓ Question 1: Can you audit password-protected content?

### Why It Matters
- Pre-launch sites are often password-protected
- Staging environments require authentication
- Member-only content affects user experience
- Gated sections contain critical product data

### Red Flags
- ❌ "We crawl public pages only"
- ❌ "You'll need to publish first"
- ❌ "We use cached data"
- ❌ Vague answers about access methods

### Green Flags
- ✅ "Provide temporary credentials—we'll crawl the full site"
- ✅ "We support HTTP auth, Shopify password protection, and member portals"
- ✅ "Complete pre-launch audits are standard"

### Searchable vs AIDI
| Searchable | AIDI |
|-----------|------|
| ❌ Public pages only | ✅ Full site with credentials |
| **Cannot audit** pre-launch stores | **Full pre-launch** analysis |
| Surface-level only | Complete deep crawl |

**AIDI Advantage:** We can audit what competitors can't access.

---

## ❓ Question 2: Are your benchmarks industry-standardized?

### Why It Matters
- Without benchmarks, scores are meaningless
- "67/100" means nothing without competitive context
- Industry averages set realistic targets
- Percentile rankings show true positioning

### Red Flags
- ❌ "We provide a score out of 100"
- ❌ "Users can customize their own tests"
- ❌ "Compare your score to past results"
- ❌ No mention of industry baselines

### Green Flags
- ✅ "You're in the 42nd percentile for your category"
- ✅ "Industry average is 72/100, top quartile is 85+"
- ✅ "We test every brand with identical methodology"
- ✅ "Statistical distribution available for 15+ industries"

### Searchable vs AIDI
| Searchable | AIDI |
|-----------|------|
| ❌ No benchmarks | ✅ Industry percentiles |
| User-defined tests = incomparable | Standardized tests = fair comparison |
| Isolated scores | Competitive context |

**AIDI Advantage:** Know exactly where you stand vs competitors.

---

## ❓ Question 3: Can you reproduce results consistently?

### Why It Matters
- Proves improvements are real, not random variance
- Validates methodology is scientific
- Enables temporal tracking (before/after)
- Allows performance attribution

### Red Flags
- ❌ "Results vary based on queries used"
- ❌ "Scores depend on user-defined prompts"
- ❌ Single-run testing (no averaging)
- ❌ "Model updates may affect results"

### Green Flags
- ✅ "Locked test framework—same every time"
- ✅ "3-run average with confidence intervals"
- ✅ "Version-controlled AI models"
- ✅ "±3-5 point margin of error (95% CI)"

### Searchable vs AIDI
| Searchable | AIDI |
|-----------|------|
| ⚠️ Varies by user prompts | ✅ Locked methodology |
| Single runs | Multi-run averaging |
| Inconsistent over time | Temporally consistent |

**AIDI Advantage:** Prove improvements with statistical confidence.

---

## ❓ Question 4: Do you control for branded query bias?

### Why It Matters
- Real buyers don't search for "[your brand]"
- Branded queries inflate scores artificially
- True competitiveness requires unbranded testing
- Generic category queries reflect actual buyer behavior

### Red Flags
- ❌ "Test any queries you want"
- ❌ Examples show branded prompts
- ❌ "See how AI describes your brand"
- ❌ No mention of bias control

### Green Flags
- ✅ "Primary tests use unbranded, category-generic queries"
- ✅ "Real buyer language: 'best [category] for [use case]'"
- ✅ "Branded queries tested separately (not in main score)"
- ✅ "Bias elimination protocols documented"

### Searchable vs AIDI
| Searchable | AIDI |
|-----------|------|
| ❌ User prompts (often branded) | ✅ Systematic bias elimination |
| "best Nike shoes" ← Brand IN prompt | "best running shoes" ← Brand must earn mention |
| Confirmation bias | Scientific controls |

**AIDI Advantage:** Test how you actually compete, not how you hope to.

---

## ❓ Question 5: Is your methodology peer-reviewable?

### Why It Matters
- Audit trail for compliance/legal
- Validates scientific rigor
- Enables third-party verification
- Builds executive confidence

### Red Flags
- ❌ "Proprietary algorithm"
- ❌ "Trust our AI"
- ❌ Black box scoring
- ❌ No published methodology

### Green Flags
- ✅ "Methodology published and open"
- ✅ "Data scientists can validate our approach"
- ✅ "Full audit trail of prompts and responses"
- ✅ "Academic partnerships/peer review"

### Searchable vs AIDI
| Searchable | AIDI |
|-----------|------|
| ⚠️ Proprietary (cannot validate) | ✅ Published methodology |
| Black box scoring | Transparent framework |
| Self-reported accuracy | Peer-reviewable |

**AIDI Advantage:** Board-ready, audit-defensible results.

---

## 📊 Scorecard: Grade Your AEO Tool

| Question | Weight | Your Tool Score<br>(0-10) | AIDI Score |
|----------|--------|--------------------------|------------|
| 1. Password-protected access | 20% | _____ | 10/10 |
| 2. Industry benchmarks | 25% | _____ | 10/10 |
| 3. Reproducible methodology | 20% | _____ | 10/10 |
| 4. Bias control | 20% | _____ | 10/10 |
| 5. Peer-reviewable | 15% | _____ | 10/10 |
| **TOTAL** | 100% | **_____/10** | **10/10** |

### Interpretation
- **9-10:** Audit-grade (enterprise-ready)
- **7-8:** Strong (suitable for most strategic decisions)
- **5-6:** Adequate (good for tactical monitoring)
- **<5:** Monitoring-grade only (not for strategic use)

---

## 💡 What This Means for Your Decision

### If Your Current Tool Scores <7:
**Consider:**
- Use it for daily tactical monitoring (not strategic planning)
- Don't base board presentations on these results
- Supplement with audit-grade evaluation (AIDI) for key decisions

### If You're Evaluating Multiple Vendors:
**Test Them:**
1. Ask all 5 questions in RFP
2. Request methodology documentation
3. Ask for sample industry benchmarks
4. Verify reproducibility claims
5. Check for peer-reviewed publications

### If You Need Enterprise-Grade Results:
**Requirements:**
- Full site access (including protected content)
- Industry benchmarking (not isolated scores)
- Statistical validation (confidence intervals, p-values)
- Bias-free testing (unbranded queries)
- Audit trail (methodology documentation)

**Only AIDI currently meets all 5 criteria.**

---

## 🎯 Real-World Scenarios

### Scenario 1: Board Presentation
**Question:** Can you defend these results to the CFO?

**Monitoring-Grade Tool:**
- "We scored 67/100"
- CFO: "What does that mean? How do we compare?"
- You: "Um..."
- Result: ❌ Budget not approved

**Audit-Grade Tool (AIDI):**
- "We're in the 42nd percentile, trailing category leaders by 18 points"
- CFO: "What's the improvement plan?"
- You: "Here's our prioritized roadmap with expected ROI"
- Result: ✅ $500K budget approved

---

### Scenario 2: Pre-Acquisition Due Diligence
**Question:** Can you audit the target before purchase?

**Monitoring-Grade Tool:**
- Target site is password-protected (common for M&A)
- Tool: "Cannot access—please publish first"
- You: ❌ Incomplete due diligence
- Result: Overpay or pass on deal

**Audit-Grade Tool (AIDI):**
- Provide seller credentials
- Complete audit of protected site
- Discover $2M in hidden AEO gaps
- Result: ✅ Renegotiate price

---

### Scenario 3: Performance Attribution
**Question:** Can you prove your AEO program worked?

**Monitoring-Grade Tool:**
- Before: Custom prompt set A
- After: Custom prompt set B (you changed them)
- "Score improved 10 points!"
- CFO: "Could this be random variance?"
- You: ❌ Cannot prove causation
- Result: Program not renewed

**Audit-Grade Tool (AIDI):**
- Before: Standardized test framework
- After: Same test framework (locked)
- "+12 points (p=0.03, statistically significant)"
- CFO: "What's the confidence interval?"
- You: "95% CI: +8 to +16 points"
- Result: ✅ $200K annual program approved

---

## 📋 Printable Checklist for Procurement

```
Enterprise AEO Tool Evaluation Checklist

□ Can audit password-protected/staging content
□ Provides industry-specific benchmarks
□ Offers percentile rankings (not just scores)
□ Uses reproducible, locked test framework
□ Provides multi-run averaging (not single runs)
□ Reports confidence intervals and p-values
□ Controls for branded query bias
□ Uses unbranded, generic category queries
□ Has published, peer-reviewable methodology
□ Provides full audit trail (prompts + responses)
□ Offers API access for BI integration
□ Supports enterprise SLAs
□ Includes competitive comparison features
□ Validated by third-party data scientists
□ Used for board-level presentations

✅ AIDI meets all 15 criteria
❌ Searchable meets 3/15 (monitoring use only)
```

---

## 🚀 Next Steps

### For Prospects Evaluating Tools:
1. **Download this framework** - Use in your RFP process
2. **Ask all vendors** - Compare responses objectively
3. **Request demos** - Verify claims in real-time
4. **Check references** - Ask about audit-grade use cases

### For AIDI Sales Conversations:
1. **Send this document** - Let prospects self-qualify
2. **Walk through questions** - Highlight gaps in current tools
3. **Show methodology** - Demonstrate audit-grade rigor
4. **Provide sample reports** - Prove industry benchmarking

### For Marketing:
1. **Publish as lead magnet** - "5 Questions to Ask Your AEO Tool"
2. **LinkedIn content series** - One post per question
3. **Sales enablement** - Training deck for team
4. **Website resource** - Comparison guide download

---

## 📞 How AIDI Answers Each Question

### Question 1: Password-protected access?
**AIDI:** "Yes. Provide credentials—we'll audit the complete site including staging environments, member portals, and gated content."

### Question 2: Industry-standardized benchmarks?
**AIDI:** "Yes. We provide percentile rankings across 15+ industries with statistical distributions. You'll know exactly where you stand vs competitors."

### Question 3: Reproducible results?
**AIDI:** "Yes. Locked test framework with version-controlled models. 3-run averaging with 95% confidence intervals (±3-5 points)."

### Question 4: Branded query bias control?
**AIDI:** "Yes. Primary tests use unbranded, category-generic queries that reflect real buyer behavior. Branded queries tested separately."

### Question 5: Peer-reviewable methodology?
**AIDI:** "Yes. Framework published, data scientist-validated, academic partnerships. Full audit trail of every prompt and response."

---

## 💬 The Closing Question

**After prospects evaluate their current tool:**

> "Now that you've scored your current AEO tool against these 5 questions, let me ask:
>
> If you were presenting AEO strategy to your board next quarter, would you feel confident defending results from a tool that scores below 7/10 on enterprise-readiness?
>
> Or would you want audit-grade validation from a platform specifically designed for strategic decisions?
>
> AIDI is the only AEO intelligence platform that answers 'yes' to all 5 questions.
>
> When the decision matters, audit-grade rigor matters.
>
> Shall we schedule your evaluation?"

---

**Status:** ✅ Ready for Sales Enablement  
**Format:** Printable one-pager, email template, LinkedIn series  
**Audience:** Enterprise buyers, procurement, data scientists  
**Goal:** Qualify prospects and differentiate from monitoring-grade tools

**USE THIS FRAMEWORK TO WIN ENTERPRISE DEALS AGAINST SEARCHABLE.**

